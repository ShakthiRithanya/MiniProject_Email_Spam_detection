{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"padding: 2.5rem; border-radius: 1.5rem; background: linear-gradient(135deg, #1e293b, #0f172a); color: #f8fafc; border: 1px solid rgba(255,255,255,0.1);\">\n",
                "    <h1 style=\"color: #38bdf8; font-size: 3rem; font-weight: 800; margin: 0;\">AI Spam Sentry: Engine Development</h1>\n",
                "    <p style=\"color: #94a3b8; font-size: 1.2rem; margin-top: 0.5rem;\">High-Performance Neural-Linguistic Pipeline for Secure Messaging</p>\n",
                "    <div style=\"margin-top: 1.5rem; font-size: 0.9rem; opacity: 0.8;\">\n",
                "        <span>Developed by: ADVERK Intelligence Team</span> | \n",
                "        <span>Framework: Scikit-Learn + NLTK</span>\n",
                "    </div>\n",
                "</div>\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Orchestration\n",
                "We initialize our environment with specialized libraries for vectorized text processing and statistical modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import joblib\n",
                "import string\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
                "\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "nltk.download('stopwords')\n",
                "\n",
                "print(\"âœ… Core Libraries Operational\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Ingestion & Transformation\n",
                "Retrieving the SMS Spam Collection dataset and restructuring for supervised learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset from high-availability repository\n",
                "url = \"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\"\n",
                "df = pd.read_csv(url, encoding='latin-1')\n",
                "\n",
                "# Schema refining\n",
                "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
                "df.columns = ['label', 'message']\n",
                "\n",
                "print(f\"Dataset Statistics: {df.shape[0]} samples processed.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Linguistic Preprocessing Pipeline\n",
                "A custom-built function to normalize text data, removing noise while preserving semantics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "def extract_linguistic_features(text):\n",
                "    # Deep normalization\n",
                "    text = text.lower()\n",
                "    text = ''.join([char for char in text if char not in string.punctuation])\n",
                "    \n",
                "    # Token purification\n",
                "    words = text.split()\n",
                "    words = [word for word in words if word not in stop_words]\n",
                "    return ' '.join(words)\n",
                "\n",
                "df['processed_message'] = df['message'].apply(extract_linguistic_features)\n",
                "print(\"âœ… Pipeline Phase 1: Normalization Complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Vectorization: TF-IDF Embedding\n",
                "Converting textual information into high-dimensional numerical space using Term Frequency-Inverse Document Frequency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df['processed_message'], \n",
                "    df['label_num'], \n",
                "    test_size=0.3, \n",
                "    random_state=42, \n",
                "    stratify=df['label_num']\n",
                ")\n",
                "\n",
                "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1,2))\n",
                "X_train_tfidf = tfidf.fit_transform(X_train)\n",
                "X_test_tfidf = tfidf.transform(X_test)\n",
                "\n",
                "print(f\"Vector Space Dimension: {X_train_tfidf.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Synthesis: Logistic Regression\n",
                "Training a discriminative classifier optimized for high-dimensional text classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classifier = LogisticRegression(C=1.0, solver='liblinear')\n",
                "classifier.fit(X_train_tfidf, y_train)\n",
                "\n",
                "y_pred = classifier.predict(X_test_tfidf)\n",
                "print(\"âœ… Model Optimization Successful\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Comprehensive Performance Audit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"CORE ANALYTICS REPORT\")\n",
                "print(\"=====================\")\n",
                "print(f\"Precision Excellence: {precision_score(y_test, y_pred):.4f}\")\n",
                "print(f\"F1 Harmonization:     {f1_score(y_test, y_pred):.4f}\")\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='inferno')\n",
                "plt.title('Error Matrix Distribution', fontsize=15)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Artifact Serialization\n",
                "Saving the trained architecture for real-time inference in the Sentry App."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "joblib.dump(classifier, 'spam_model.pkl')\n",
                "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
                "print(\"ðŸ“¦ Artifacts cached in 2_Mini_Spam_Email_Detection/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}